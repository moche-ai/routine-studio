"""
íŠ¸ë Œë“œ ê¸°ë°˜ YouTube ì±„ë„ ë”¥ë¦¬ì„œì¹˜ ì„œë¹„ìŠ¤
ì±„ë„ëª…/ë¶„ì•¼ ê¸°ë°˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìƒì„± + ì±„ë„ ìŠ¤ì½”ì–´ë§ + ìë§‰ ë¶„ì„
"""
import asyncio
import json
import re
import requests
import subprocess
from datetime import datetime
from typing import List, Dict, Optional
from pathlib import Path

try:
    from playwright.async_api import async_playwright
except ImportError:
    pass

SCREENSHOT_DIR = Path("/data/routine/routine-studio-v2/screenshots/youtube")
LLM_URL = "http://localhost:8017/v1/chat/completions"


class TrendYouTubeResearcher:
    def __init__(self):
        self.browser = None
        SCREENSHOT_DIR.mkdir(parents=True, exist_ok=True)

    async def _ensure_browser(self):
        if self.browser is None:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(headless=True)
        return self.browser

    def generate_trending_keywords(self, channel_name: str, field: str, count: int = 5) -> List[str]:
        """ì±„ë„ëª…ê³¼ ë¶„ì•¼ ê¸°ë°˜ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìƒì„±"""
        prompt = f"""'{channel_name}' ì±„ë„ì˜ '{field}' ë¶„ì•¼ì—ì„œ í˜„ì¬ íŠ¸ë Œë“œì¸ YouTube ê²€ìƒ‰ í‚¤ì›Œë“œ {count}ê°œë¥¼ ìƒì„±í•´ì¤˜.

ìš”êµ¬ì‚¬í•­:
- ì‹¤ì œ YouTubeì—ì„œ ê²€ìƒ‰ëŸ‰ì´ ë†’ì„ ë§Œí•œ í‚¤ì›Œë“œ
- í•´ë‹¹ ë¶„ì•¼ì˜ ìµœì‹  íŠ¸ë Œë“œ ë°˜ì˜
- êµ¬ì²´ì ì´ê³  ê²€ìƒ‰í•˜ê¸° ì¢‹ì€ í˜•íƒœ
- í•œêµ­ì–´ í‚¤ì›Œë“œ

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}"""

        try:
            resp = requests.post(LLM_URL, json={
                'model': 'gpt-oss-120b-longctx',
                'messages': [{'role': 'user', 'content': prompt}],
                'max_tokens': 512,
                'temperature': 0.7
            }, timeout=60)

            if resp.status_code == 200:
                content = resp.json()['choices'][0]['message']['content']
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0]
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0]
                result = json.loads(content.strip())
                return result.get('keywords', [field])
            return [field]
        except Exception as e:
            print(f"Keyword generation error: {e}")
            return [field]

    def _parse_count(self, text: str) -> int:
        if not text:
            return 0
        text = text.replace(',', '').replace(' ', '')
        multipliers = {'ë§Œ': 10000, 'ì²œ': 1000, 'K': 1000, 'M': 1000000}
        for suffix, mult in multipliers.items():
            if suffix in text:
                num = re.findall(r'[\d.]+', text)
                if num:
                    return int(float(num[0]) * mult)
        nums = re.findall(r'\d+', text)
        return int(nums[0]) if nums else 0

    def _parse_days_ago(self, text: str) -> Optional[int]:
        if not text:
            return None
        patterns = [
            (r'(\d+)\s*ë¶„\s*ì „', lambda x: 0),
            (r'(\d+)\s*ì‹œê°„\s*ì „', lambda x: 0),
            (r'(\d+)\s*ì¼\s*ì „', lambda x: int(x)),
            (r'(\d+)\s*ì£¼\s*ì „', lambda x: int(x) * 7),
            (r'(\d+)\s*ê°œì›”\s*ì „', lambda x: int(x) * 30),
            (r'(\d+)\s*ë…„\s*ì „', lambda x: int(x) * 365),
        ]
        for pattern, converter in patterns:
            match = re.search(pattern, text)
            if match:
                return converter(match.group(1))
        return None

    def download_transcript(self, video_url: str) -> Optional[str]:
        """yt-dlpë¡œ ìë§‰ ë‹¤ìš´ë¡œë“œ"""
        try:
            video_id = None
            if 'v=' in video_url:
                video_id = video_url.split('v=')[1].split('&')[0]
            elif 'youtu.be/' in video_url:
                video_id = video_url.split('youtu.be/')[1].split('?')[0]

            if not video_id:
                return None

            output_path = SCREENSHOT_DIR / f"transcript_{video_id}"

            cmd = [
                'yt-dlp',
                '--skip-download',
                '--write-auto-sub',
                '--sub-lang', 'ko,en',
                '--sub-format', 'vtt',
                '-o', str(output_path),
                f'https://www.youtube.com/watch?v={video_id}'
            ]

            subprocess.run(cmd, capture_output=True, text=True, timeout=30)

            for ext in ['.ko.vtt', '.en.vtt', '.vtt']:
                vtt_path = Path(str(output_path) + ext)
                if vtt_path.exists():
                    content = vtt_path.read_text(encoding='utf-8')
                    lines = []
                    for line in content.split('\n'):
                        if not re.match(r'^\d{2}:\d{2}', line) and not line.startswith('WEBVTT') and line.strip():
                            lines.append(line.strip())
                    return ' '.join(lines)[:5000]

            return None
        except Exception as e:
            print(f"Transcript error: {e}")
            return None

    async def get_channel_with_videos(self, channel_url: str) -> Dict:
        """ì±„ë„ ì •ë³´ + ìµœê·¼ ì˜ìƒ ìˆ˜ì§‘"""
        browser = await self._ensure_browser()
        page = await browser.new_page()

        try:
            videos_url = channel_url.rstrip('/') + '/videos'
            await page.goto(videos_url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(2)

            data = await page.evaluate('''
                () => {
                    const result = {
                        name: '',
                        subscribers: '',
                        videos: []
                    };

                    const nameEl = document.querySelector('#channel-name');
                    if (nameEl) result.name = nameEl.textContent.trim();

                    const subsEl = document.querySelector('#subscriber-count');
                    if (subsEl) result.subscribers = subsEl.textContent.trim();

                    const items = document.querySelectorAll('ytd-rich-item-renderer');
                    for (let i = 0; i < Math.min(items.length, 10); i++) {
                        const item = items[i];
                        const titleEl = item.querySelector('#video-title');
                        const linkEl = item.querySelector('#video-title-link');
                        const viewsEl = item.querySelector('#metadata-line span:first-child');
                        const dateEl = item.querySelector('#metadata-line span:last-child');

                        if (titleEl) {
                            result.videos.push({
                                title: titleEl.textContent.trim(),
                                url: linkEl ? linkEl.href : '',
                                views: viewsEl ? viewsEl.textContent.trim() : '',
                                date: dateEl ? dateEl.textContent.trim() : ''
                            });
                        }
                    }
                    return result;
                }
            ''')

            return {**data, 'url': channel_url}
        finally:
            await page.close()

    def calculate_score(self, channel: Dict) -> Dict:
        """ì±„ë„ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        scores = {'subscriber': 0, 'activity': 0, 'engagement': 0, 'growth': 0, 'total': 0}

        subs = self._parse_count(channel.get('subscribers', ''))

        # êµ¬ë…ì ì ìˆ˜ (30ì )
        if subs >= 1000000: scores['subscriber'] = 30
        elif subs >= 500000: scores['subscriber'] = 27
        elif subs >= 100000: scores['subscriber'] = 24
        elif subs >= 50000: scores['subscriber'] = 20
        elif subs >= 10000: scores['subscriber'] = 15
        else: scores['subscriber'] = 10

        # í™œë™ì„± ì ìˆ˜ (25ì )
        videos = channel.get('videos', [])
        if videos:
            days = [self._parse_days_ago(v.get('date', '')) for v in videos[:5]]
            days = [d for d in days if d is not None]
            if days:
                avg = sum(days) / len(days)
                if avg <= 7: scores['activity'] = 25
                elif avg <= 14: scores['activity'] = 20
                elif avg <= 30: scores['activity'] = 15
                else: scores['activity'] = 10

        # ì°¸ì—¬ë„ ì ìˆ˜ (25ì )
        if videos and subs > 0:
            views = [self._parse_count(v.get('views', '')) for v in videos[:5]]
            views = [v for v in views if v > 0]
            if views:
                ratio = (sum(views) / len(views)) / subs
                if ratio >= 0.3: scores['engagement'] = 25
                elif ratio >= 0.1: scores['engagement'] = 20
                elif ratio >= 0.05: scores['engagement'] = 15
                else: scores['engagement'] = 10

        # ì„±ì¥ ì ìˆ˜ (20ì )
        if len(videos) >= 6:
            recent = [self._parse_count(v.get('views', '')) for v in videos[:3]]
            older = [self._parse_count(v.get('views', '')) for v in videos[3:6]]
            r_avg = sum(recent) / len(recent) if recent else 0
            o_avg = sum(older) / len(older) if older else 1
            if o_avg > 0:
                growth = (r_avg - o_avg) / o_avg
                if growth >= 0.5: scores['growth'] = 20
                elif growth >= 0.2: scores['growth'] = 15
                elif growth >= 0: scores['growth'] = 10
                else: scores['growth'] = 5

        scores['total'] = scores['subscriber'] + scores['activity'] + scores['engagement'] + scores['growth']
        return scores

    async def search_channels(self, keyword: str, max_channels: int = 8) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ì±„ë„ ê²€ìƒ‰ ë° ìŠ¤ì½”ì–´ë§"""
        browser = await self._ensure_browser()
        page = await browser.new_page()

        try:
            url = f'https://www.youtube.com/results?search_query={keyword}&sp=EgIQAg%253D%253D'
            await page.goto(url, wait_until='networkidle', timeout=30000)
            await asyncio.sleep(3)

            channels = await page.evaluate(f'''
                () => {{
                    const items = document.querySelectorAll('ytd-channel-renderer');
                    return Array.from(items).slice(0, {max_channels}).map(item => {{
                        const nameEl = item.querySelector('#text-container yt-formatted-string');
                        const linkEl = item.querySelector('#main-link');
                        const subsEl = item.querySelector('#subscribers');
                        return {{
                            name: nameEl ? nameEl.textContent.trim() : '',
                            url: linkEl ? linkEl.href : '',
                            subscribers: subsEl ? subsEl.textContent.trim() : ''
                        }};
                    }}).filter(c => c.url);
                }}
            ''')
            await page.close()

            results = []
            for ch in channels:
                try:
                    details = await self.get_channel_with_videos(ch['url'])
                    details['name'] = ch['name'] or details.get('name', '')
                    details['subscribers'] = ch['subscribers'] or details.get('subscribers', '')
                    details['scores'] = self.calculate_score(details)
                    details['keyword'] = keyword
                    results.append(details)
                except Exception as e:
                    ch['error'] = str(e)
                    ch['scores'] = {'total': 0}
                    ch['keyword'] = keyword
                    results.append(ch)

            return results
        except Exception as e:
            return [{'error': str(e), 'keyword': keyword}]

    def analyze_with_llm(self, channels: List[Dict], channel_name: str, field: str) -> Dict:
        """LLMìœ¼ë¡œ ë¶„ì„ ë° ì„ ì •"""
        summaries = []
        for i, ch in enumerate(channels[:10], 1):
            s = ch.get('scores', {})
            videos = [v.get('title', '')[:40] for v in ch.get('videos', [])[:3]]
            summaries.append(f"""{i}. {ch.get('name', '?')} ({ch.get('subscribers', '?')}) [í‚¤ì›Œë“œ: {ch.get('keyword', '?')}]
   ì ìˆ˜: {s.get('total', 0)}/100 (êµ¬ë…{s.get('subscriber',0)} í™œë™{s.get('activity',0)} ì°¸ì—¬{s.get('engagement',0)} ì„±ì¥{s.get('growth',0)})
   ì˜ìƒ: {', '.join(videos)}""")

        prompt = f"""'{channel_name}' ì±„ë„ì˜ '{field}' ë¶„ì•¼ ë ˆí¼ëŸ°ìŠ¤ ì±„ë„ì„ ì„ ì •í•´ì¤˜.

ê²€ìƒ‰ëœ ì±„ë„ë“¤:
{chr(10).join(summaries)}

ìš”êµ¬ì‚¬í•­:
1. ë ˆí¼ëŸ°ìŠ¤ë¡œ ì‚¼ê¸° ì¢‹ì€ ìƒìœ„ 5ê°œ ì„ ì •
2. ì½˜í…ì¸  ìŠ¤íƒ€ì¼, ê°•ì , ë°°ìš¸ ì  ë¶„ì„
3. ì±„ë„ë³„ íŠ¹ì§•ê³¼ ì°¨ë³„ì  ì„¤ëª…

JSON í˜•ì‹:
{{"top5": [{{"rank": 1, "name": "ì±„ë„ëª…", "reason": "ì„ ì • ì´ìœ ", "content_style": "ì½˜í…ì¸  ìŠ¤íƒ€ì¼", "key_learning": "ë°°ìš¸ ì ", "differentiation": "ì°¨ë³„ì "}}], "overall_insight": "ì „ì²´ ì¸ì‚¬ì´íŠ¸", "content_strategy": "ì¶”ì²œ ì½˜í…ì¸  ì „ëµ"}}"""

        try:
            resp = requests.post(LLM_URL, json={
                'model': 'gpt-oss-120b-longctx',
                'messages': [{'role': 'user', 'content': prompt}],
                'max_tokens': 2048
            }, timeout=120)

            if resp.status_code == 200:
                content = resp.json()['choices'][0]['message']['content']
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0]
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0]
                return json.loads(content.strip())
            return {'error': f'LLM {resp.status_code}'}
        except Exception as e:
            return {'error': str(e)}

    async def deep_research(self, channel_name: str, field: str, max_keywords: int = 5) -> Dict:
        """íŠ¸ë Œë“œ ê¸°ë°˜ ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰"""
        print(f'=== íŠ¸ë Œë“œ ê¸°ë°˜ YouTube ë”¥ë¦¬ì„œì¹˜ ===')
        print(f'ì±„ë„: {channel_name} | ë¶„ì•¼: {field}\n')

        # 1. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìƒì„±
        print('1. íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìƒì„± ì¤‘...')
        keywords = self.generate_trending_keywords(channel_name, field, max_keywords)
        print(f'   ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}\n')

        # 2. ê° í‚¤ì›Œë“œë¡œ ì±„ë„ ê²€ìƒ‰
        print('2. ì±„ë„ ê²€ìƒ‰ ë° ìŠ¤ì½”ì–´ë§...')
        all_channels = []
        for kw in keywords:
            print(f'   ê²€ìƒ‰: {kw}')
            channels = await self.search_channels(kw, max_channels=5)
            all_channels.extend(channels)

        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_channels = []
        for ch in all_channels:
            url = ch.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_channels.append(ch)

        # ìŠ¤ì½”ì–´ ê¸°ì¤€ ì •ë ¬
        unique_channels.sort(key=lambda x: x.get('scores', {}).get('total', 0), reverse=True)

        print(f'\n   ì´ {len(unique_channels)}ê°œ ì±„ë„ ë°œê²¬\n')
        for ch in unique_channels[:10]:
            s = ch.get('scores', {})
            print(f"   {ch.get('name', '?')}: {s.get('total', 0)}ì  ({ch.get('subscribers', 'N/A')})")

        # 3. LLM ë¶„ì„
        print('\n3. LLM ë¶„ì„ ì¤‘...')
        analysis = self.analyze_with_llm(unique_channels[:10], channel_name, field)

        if 'top5' in analysis:
            print('\n=== ì„ ì •ëœ ë ˆí¼ëŸ°ìŠ¤ ì±„ë„ ===')
            for ch in analysis['top5']:
                print(f"\n{ch.get('rank')}ìœ„: {ch.get('name')}")
                print(f"   ì´ìœ : {ch.get('reason')}")
                print(f"   ìŠ¤íƒ€ì¼: {ch.get('content_style')}")
                print(f"   ë°°ìš¸ ì : {ch.get('key_learning')}")
            print(f"\nì¸ì‚¬ì´íŠ¸: {analysis.get('overall_insight', '')}")
            print(f"ì „ëµ: {analysis.get('content_strategy', '')}")

        # 4. ìƒìœ„ ì±„ë„ ìë§‰ ìˆ˜ì§‘
        print('\n4. ìƒìœ„ ì±„ë„ ì˜ìƒ ìë§‰ ìˆ˜ì§‘...')
        for ch in unique_channels[:3]:
            videos = ch.get('videos', [])[:2]
            for v in videos:
                if v.get('url'):
                    print(f"   ìë§‰: {v.get('title', '')[:30]}...")
                    transcript = self.download_transcript(v['url'])
                    if transcript:
                        v['transcript'] = transcript[:2000]
                        print(f"      -> {len(transcript)} chars")

        # 5. ê²°ê³¼ ì €ì¥
        result = {
            'channel_name': channel_name,
            'field': field,
            'keywords': keywords,
            'channels': unique_channels,
            'analysis': analysis,
            'timestamp': datetime.now().isoformat()
        }

        output = SCREENSHOT_DIR / f'trend_research_{channel_name.replace(" ", "_")}.json'
        with open(output, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f'\nì €ì¥: {output}')

        return result

    def generate_script_prompt(self, channel_name: str, video_idea: Dict) -> str:
        """ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        title = video_idea.get('title', '')
        hook = video_idea.get('hook', '')
        summary = video_idea.get('summary', '')

        # ì•„ì´ë””ì–´ ìµœì í™” (ì œëª© + í›„í‚¹ + ìš”ì•½ ê²°í•©)
        optimized_idea = f"{title}"
        if hook:
            optimized_idea += f"\n\ní•µì‹¬ ë©”ì‹œì§€: {hook}"
        if summary:
            optimized_idea += f"\n\në‚´ìš© ë°©í–¥: {summary}"

        prompt = f'''ì´ ì œëª©ìœ¼ë¡œ {channel_name}ì˜ ë…íŠ¹í•œ ê¸ˆìœµ êµìœ¡ ìŠ¤íƒ€ì¼ì„ ì‚´ë ¤ì„œ ìœ íŠœë¸Œ ëŒ€ë³¸ì„ ì¨ì¤˜:
"{optimized_idea}"

ëŒ€ë³¸ì€ {channel_name}ì´ ì“°ëŠ” ê²€ì¦ëœ êµ¬ì¡°ë¥¼ ê¼­ ì§€ì¼œì•¼ í•´:

ì˜¤í”„ë‹ í›…: ì‹œì²­ìê°€ ë³´ìë§ˆì "ì´ê±° ë‚´ ì–˜ê¸´ë°?"ë¼ê³  ìƒê°í•  ë§Œí•œ ê³µê° ê°€ëŠ” ìƒí™©ìœ¼ë¡œ ì‹œì‘í•´.

ë„ì…ë¶€: ì´ ë¬¸êµ¬ë¥¼ ê¼­ ë„£ì–´:
"ë‚´ ì´ë¦„ì€ {channel_name}ì•¼. ë‚œ [ê´€ë ¨ ê¸ˆìœµ ì£¼ì œ]ì— ëŒ€í•´ì„œ ì§„ì§œ ë¯¸ì¹œ ë“¯ì´ ê³ ë¯¼í•˜ê±°ë“ .
ë§Œì•½ ë„¤ê°€ [ì‹œì²­ìì˜ ê³ ë¯¼ ë¬˜ì‚¬] ë•Œë¬¸ì— í˜ë“¤ë‹¤ë©´, êµ¬ë… ë²„íŠ¼ ëˆ„ë¥´ê³  ì´ ì˜ìƒì´ ë„ì›€ ë˜ë©´ ì¢‹ì•„ìš”ë„ ê¼­ ëˆŒëŸ¬ì¤˜."

ğŸ’¡ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
- ì¹œêµ¬ë‘ ë§í•˜ë“¯ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì¨. ("ì‚¬ì‹¤ ë§ì´ì•¼...", "ì ë´ë´", "ìŒ" ê°™ì€ í‘œí˜„ í™œìš©)
- ì˜ìƒ ì´ˆë°˜ì— ì‚¬ëŒë“¤ì´ í”íˆ ë¯¿ëŠ” ê¸ˆìœµ ìƒì‹ì„ ê¹¨ë¶€ìˆ´ì¤˜.
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì¶œì²˜ì˜ ë†€ë¼ìš´ í†µê³„ ìˆ˜ì¹˜ë¥¼ ë„£ì–´ì¤˜.
- ì‹¤ì œ ê¸ˆì•¡ì„ ì¨ì„œ ìˆ˜í•™ì ìœ¼ë¡œ ë”±ë”± ê³„ì‚°ë˜ëŠ” ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤˜.
- ì™œ ê·¸ëŸ° ê²½ì œì  ê²°ì •ì„ ë‚´ë¦¬ëŠ”ì§€ ì‹¬ë¦¬ì ì¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì¤˜.
- ì‹œì²­ìê°€ ë°˜ë°•í•  ë§Œí•œ ë‚´ìš©ì— ë¯¸ë¦¬ ëŒ€ë‹µí•´ì¤˜. ("ì§€ê¸ˆ ì´ëŸ° ìƒê° ë“¤ì§€?" ë“±)
- ì–´ë ¤ìš´ ê°œë…ì€ ì‰¬ìš´ ë¹„ìœ ë¥¼ ë“¤ì–´ì„œ ì„¤ëª…í•´.
- ë’¤ë¡œ ê°ˆìˆ˜ë¡ ë” ë†€ë¼ìš´ í†µì°°ë ¥ì„ ë³´ì—¬ì¤˜ì•¼ í•´.
- "ì‚¬ëŒë“¤ì´ ì§„ì§œ ëª¨ë¥´ëŠ” ê²Œ ë­ëƒë©´...", "ì—¬ê¸°ì„œë¶€í„° ì§„ì§œ ì¬ë°Œì–´ì§„ë‹¤..." ê°™ì€ í‘œí˜„ì„ ì¨ì¤˜.

ğŸ“‹ ì½˜í…ì¸  ìš”êµ¬ ì‚¬í•­
- ë¶„ëŸ‰: 10~15ë¶„ ì •ë„ ì˜ìƒì— ë§ëŠ” ê¸¸ì´.
- ëˆì˜ ê³„ì‚°ì ì¸ ë¶€ë¶„ê³¼ ì‹¬ë¦¬ì ì¸ ë¶€ë¶„ì˜ ê· í˜•ì„ ë§ì¶°ì¤˜.
- ì‹œì²­ìê°€ ë°”ë¡œ ë”°ë¼ í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë‹¨ê³„ë¥¼ ì•Œë ¤ì¤˜.
- ì¤‘ìš”í•œ í¬ì¸íŠ¸ëŠ” ì´ì•¼ê¸° í˜•ì‹(ìŠ¤í† ë¦¬í…”ë§)ìœ¼ë¡œ í’€ì–´ì¤˜.
- ë§ˆì§€ë§‰ì—” ê°•ë ¥í•œ ë™ê¸°ë¶€ì—¬ì™€ í•¨ê»˜ êµ¬ë…/ì¢‹ì•„ìš”ë¥¼ ìœ ë„í•˜ë©° ëë‚´ì¤˜.

ğŸ­ ë§íˆ¬ (í†¤ì•¤ë§¤ë„ˆ)
ê¶Œìœ„ëŠ” ìˆì§€ë§Œ ì¹œê·¼í•˜ê²Œ. ì‚¬ëŒë“¤ì´ ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ì— ëŒ€í•´ ì•½ê°„ ë‹µë‹µí•´í•˜ë©´ì„œë„,
ì§„ì‹¬ìœ¼ë¡œ ì‹œì²­ìì˜ ë¯¸ë˜ë¥¼ ê±±ì •í•´ ì£¼ëŠ” ëŠë‚Œì„ ìœ ì§€í•´ì¤˜.'''

        return prompt

    def generate_video_ideas_with_prompts(self, channel_name: str, field: str, reference_channels: List[Dict], count: int = 20) -> Dict:
        """ë ˆí¼ëŸ°ìŠ¤ ì±„ë„ ê¸°ë°˜ ì˜ìƒ ì•„ì´ë””ì–´ + ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # ë ˆí¼ëŸ°ìŠ¤ ì±„ë„ ì˜ìƒ ì œëª©ë“¤ ìˆ˜ì§‘
        ref_titles = []
        for ch in reference_channels[:5]:
            for v in ch.get('videos', [])[:3]:
                ref_titles.append(v.get('title', ''))

        prompt = f"""'{channel_name}' ì±„ë„ì˜ '{field}' ë¶„ì•¼ ë…ì°½ì ì¸ ì˜ìƒ ì•„ì´ë””ì–´ {count}ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.

ë ˆí¼ëŸ°ìŠ¤ ì±„ë„ ì¸ê¸° ì˜ìƒ:
{chr(10).join(['- ' + t for t in ref_titles if t])}

ìš”êµ¬ì‚¬í•­:
- ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì°¸ê³ í•˜ë˜ ì°¨ë³„í™”ëœ ë…ì°½ì ì¸ ì•„ì´ë””ì–´
- í´ë¦­ì„ ìœ ë„í•˜ëŠ” ì œëª©
- ê°•ë ¥í•œ í›„í‚¹ ë¬¸ì¥
- êµ¬ì²´ì ì¸ ë‚´ìš© ìš”ì•½

JSON í˜•ì‹:
{{"ideas": [{{"title": "ì œëª©", "hook": "í›„í‚¹ë¬¸ì¥", "summary": "ë‚´ìš©ìš”ì•½"}}]}}"""

        try:
            resp = requests.post(LLM_URL, json={
                'model': 'gpt-oss-120b-longctx',
                'messages': [{'role': 'user', 'content': prompt}],
                'max_tokens': 4096,
                'temperature': 0.8
            }, timeout=120)

            if resp.status_code == 200:
                content = resp.json()['choices'][0]['message']['content']
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0]
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0]

                ideas = json.loads(content.strip())

                # ê° ì•„ì´ë””ì–´ì— ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                for idea in ideas.get('ideas', []):
                    idea['script_prompt'] = self.generate_script_prompt(channel_name, idea)

                return ideas
            return {'error': f'LLM {resp.status_code}'}
        except Exception as e:
            return {'error': str(e)}

    async def close(self):
        if self.browser:
            await self.browser.close()


async def main():
    import sys

    # ê¸°ë³¸ê°’ ë˜ëŠ” ì¸ìë¡œ ë°›ê¸°
    channel_name = sys.argv[1] if len(sys.argv) > 1 else 'MoneyMindset'
    field = sys.argv[2] if len(sys.argv) > 2 else 'ì¬í…Œí¬ íˆ¬ì'

    researcher = TrendYouTubeResearcher()

    try:
        # 1. ë”¥ë¦¬ì„œì¹˜ ì‹¤í–‰
        result = await researcher.deep_research(channel_name, field)
        print(f"\nì™„ë£Œ! ì´ {len(result.get('channels', []))}ê°œ ì±„ë„ ë¶„ì„")

        # 2. ì˜ìƒ ì•„ì´ë””ì–´ + ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        print('\n5. ì˜ìƒ ì•„ì´ë””ì–´ + ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒì„±...')
        ideas_result = researcher.generate_video_ideas_with_prompts(
            channel_name, field, result.get('channels', []), count=20
        )

        if 'ideas' in ideas_result:
            print(f"\n=== {len(ideas_result['ideas'])}ê°œ ì˜ìƒ ì•„ì´ë””ì–´ ìƒì„± ===")
            for i, idea in enumerate(ideas_result['ideas'][:5], 1):
                print(f"\n{i}. {idea.get('title', 'N/A')}")
                print(f"   Hook: {idea.get('hook', 'N/A')}")

            # ì•„ì´ë””ì–´ ì €ì¥
            ideas_output = SCREENSHOT_DIR / f'video_ideas_{channel_name.replace(" ", "_")}.json'
            with open(ideas_output, 'w', encoding='utf-8') as f:
                json.dump(ideas_result, f, ensure_ascii=False, indent=2)
            print(f'\nì•„ì´ë””ì–´ ì €ì¥: {ideas_output}')

            # ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ ì¶œë ¥
            if ideas_result['ideas']:
                print('\n=== ì²« ë²ˆì§¸ ì•„ì´ë””ì–´ ëŒ€ë³¸ í”„ë¡¬í”„íŠ¸ ìƒ˜í”Œ ===')
                print(ideas_result['ideas'][0].get('script_prompt', '')[:500] + '...')

    finally:
        await researcher.close()


if __name__ == '__main__':
    asyncio.run(main())
